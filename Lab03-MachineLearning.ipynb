{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jp_Fkl6ZRsam"
   },
   "source": [
    "# Lab03: MLP\n",
    "\n",
    "---\n",
    "MSSV:\n",
    "\n",
    "Họ và tên: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tEdaWEjRsau"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7a50pluRsau",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import _pickle as cPickle\n",
    "\n",
    "# ignore warning ...\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcjyWU7eRsaw"
   },
   "source": [
    "## Định nghĩa các hàm\n",
    "**Hàm đọc bộ dữ liệu CIFAR-10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fUBW2S9Rsaw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load( CIFAR-10\n",
    "    return dict\n",
    "def read_data(containing_dir, num_train_batchs):\n",
    "    # Read training data\n",
    "    train_X_batchs = []\n",
    "    train_Y_batchs = []\n",
    "    for batch_idx in range(num_train_batchs):\n",
    "        batch = unpickle(containing_dir + '\\\\data_batch_' + str(batch_idx + 1))\n",
    "        train_X_batchs.append(batch['data'])\n",
    "        train_Y_batchs.append(np.array(batch['labels']).reshape(-1, 1))\n",
    "    train_X = np.vstack(train_X_batchs)\n",
    "    train_X = train_X / 255. # Normalize to [0, 1]\n",
    "    train_X = np.hstack([np.ones((len(train_X), 1)), train_X])\n",
    "    train_Y = np.vstack(train_Y_batchs)\n",
    "    \n",
    "    # Read test data\n",
    "    batch = unpickle(containing_dir + '\\\\test_batch')\n",
    "    test_X = batch['data']\n",
    "    test_X = test_X / 255. # Normalize to [0, 1]\n",
    "    test_X = np.hstack([np.ones((len(test_X), 1)), test_X])\n",
    "    test_Y = np.array(batch['labels']).reshape(-1, 1)\n",
    "    \n",
    "    return (train_X, train_Y, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8Dk_v62Rsax"
   },
   "source": [
    "**Hàm tính output và gradient của Neural Net**\n",
    "\n",
    "Ở đây, ta dùng hàm kích hoạt sigmoid ở các tầng ẩn, và hàm softmax ở tầng cuối.\n",
    "Ta có:\n",
    "\n",
    "$$ h=w^TX $$\n",
    "\n",
    "$$\\text{Sigmoid Activation: } z= \\sigma \\left(h\\right)= \\dfrac{1}{1+e^{-h}}$$\n",
    "\n",
    "$$\\text{Cross-entrophy loss: } J(w)=-\\left({ylog(z)+(1-y)log(1-z)}\\right)$$\n",
    "\n",
    "$$\\text{Chain rule: } \\dfrac{\\partial J(w)}{\\partial w}=\\dfrac{\\partial J(w)}{\\partial z} \\dfrac{\\partial z}{\\partial h}\\dfrac{\\partial h}{\\partial w}  $$\n",
    "\n",
    "$$\\dfrac{\\partial J(w)}{\\partial z}=-\\left(\\dfrac{y}{z}-\\dfrac{1-y}{1-z}\\right)=\\dfrac{z-y}{z(1-z)}$$\n",
    "\n",
    "$$\\dfrac{\\partial z}{\\partial h}=z(1-z)$$\n",
    "\n",
    "$$\\dfrac{\\partial h}{\\partial w}=X$$\n",
    "\n",
    "$$\\dfrac{\\partial J(w)}{\\partial w}=X^T(z-y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j71LfCTzRsay"
   },
   "source": [
    "**Hàm cost và gradient tầng sigmoid:**\n",
    "$$J(w)=\\dfrac{1}{N} \\sum_{n=1}^N-\\left({ylog(z)+(1-y)log(1-z)}\\right)  $$\n",
    "$$\\text{Gradient: } \\dfrac{\\partial J(w)}{\\partial w}=\\dfrac{1}{N}X^T(z-y)$$  \n",
    "\n",
    "Hàm softmax (C là số lớp): \n",
    "$$h(z_i)=\\dfrac{e^{z_i}}{\\sum_{j=1}^{C}e^{z_j}}$$\n",
    "**Hàm cost tầng softmax lúc này là:**\n",
    "$$J(w)=\\dfrac{1}{N} \\sum_{n=1}^N \\sum_{k=1}^K -(y_n)_k log \\left(h(x_n)_k\\right)  $$\n",
    "\n",
    "Sử dụng kỹ thuật one-hot-encoding mã hóa tập label y, ta có:\n",
    " $$\\text{với } (y_n)_k=    \\begin{align}\n",
    "        \\begin{cases}\n",
    "             0 \\text { nếu } y_n\\neq k\\\\\n",
    "             1 \\text { nếu } y_n= k\\\\\n",
    "        \\end{cases}\n",
    "    \\end{align} $$\n",
    "lúc này số lớp sẽ là 2 hay C=2:\n",
    "\n",
    "$$h(z_1)=\\dfrac{e^{z_1}}{\\sum_{j=1}^{2}e^{z_j}}$$\n",
    "\n",
    "$$h(z_1)=\\dfrac{e^{z_1}}{e^{z_1}+e^{z_2}}$$\n",
    "\n",
    "$$h(z_1)=\\dfrac{1}{1+e^{z_2-z_1}}$$\n",
    "\n",
    "=> Đây chính là hàm sigmoid. Hơn nữa mặc dù có 2 output softmax vẫn có thể rút gọn thành 1 output vì tổng 2 outputs luôn bằng 1\n",
    "\n",
    "**Do đó hàm cost của tầng softmax và tầng sigmoid là tương tự nhau** \n",
    "\n",
    "**=> Gradient của tầng softmax cũng tương tự của tầng sigmoid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqscJZB8Rsaz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "    '''\n",
    "    Computes sigmoid function for each element of array S.\n",
    "    '''\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "\n",
    "def gradient_sigmoid(As, Ws ,mb_X,delta, i):\n",
    "    '''\n",
    "    Computes gradient vector for sigmoid layer.\n",
    "    '''\n",
    "    return delta, grad\n",
    "\n",
    "\n",
    "def softmax(S):\n",
    "    '''\n",
    "    Computes softmax function for each row of array S.\n",
    "    '''\n",
    "    A = np.exp(S)\n",
    "    A /=A.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return A\n",
    "\n",
    "def gradient_softmax(As,mb_X,mb_Y):\n",
    "    '''\n",
    "    Computes gradient vector for softmaz layer\n",
    "    '''\n",
    "    return delta, grad\n",
    "\n",
    "def compute_nnet_outputs(Ws, X, need_all_layer_outputs):\n",
    "    '''\n",
    "    Computes the outputs of Neural Net by forward propagating X through the net.\n",
    "    '''\n",
    "    if need_all_layer_outputs:\n",
    "        return As\n",
    "    else:\n",
    "        return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXEjwp-SRsaz"
   },
   "source": [
    "**Các hàm khởi tạo tham số**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xxvqubb-Rsa0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_layer_sizes(X, Y, hid_layer_sizes):\n",
    "    num_classes = len(np.unique(Y)) # Num classes\n",
    "    layer_sizes = [X.shape[1] - 1] + hid_layer_sizes + [num_classes]\n",
    "    print('Layer Sizes: ',layer_sizes)\n",
    "    return layer_sizes\n",
    "\n",
    "\n",
    "def one_hot_encoding(Y, num_classes):\n",
    "    num_classes = len(np.unique(Y)) # Num classes\n",
    "    one_hot_Y = np.zeros((len(Y), num_classes))\n",
    "    one_hot_Y[np.arange(len(Y)), Y.reshape(-1)] = 1\n",
    "    return one_hot_Y\n",
    "\n",
    "\n",
    "def init_weight_matrix(X,Y,layer_sizes):\n",
    "    #fix random for same result when init weight matrix\n",
    "    np.random.seed(0) \n",
    "    Ws = np.array([np.random.randn(layer_sizes[i]+1 , layer_sizes[i + 1]) / np.sqrt(layer_sizes[i]+1) \n",
    "          for i in range(len(layer_sizes) - 1)]) # Init Ws\n",
    "    print('Weight matrix shape: ',Ws[0].shape,Ws[1].shape)\n",
    "    return Ws\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VKRdL35Rsa0"
   },
   "source": [
    "**Hàm huấn luyện Neural net**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqiUd7NMRsa1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def updateWeights(Ws,As,mb_X, mb_Y,  learning_rate):\n",
    "    #update weights for softmax layer\n",
    "    \n",
    "    \n",
    "    #update weights for hidden layer\n",
    "    \n",
    "    return Ws\n",
    "\n",
    "def train_nnet(X, Y, hid_layer_sizes, mb_size, learning_rate, max_epoch):\n",
    "    \n",
    "    #get layer sizes:\n",
    "    layer_sizes = compute_layer_sizes(X, Y, hid_layer_sizes)\n",
    "\n",
    "    \n",
    "    # Prepare for training\n",
    "    Ws=init_weight_matrix(X,Y,layer_sizes)\n",
    "    one_hot_Y=one_hot_encoding(Y,layer_sizes[-1])\n",
    "    \n",
    "    costs = [] # To save costs during training\n",
    "    errs = [] # To save mean binary errors during training\n",
    "    N = len(X) # Num training examples\n",
    "    rnd_idxs = list(range(N)) # Random indexes    \n",
    "    \n",
    "    # Train\n",
    "    for epoch in list(range(max_epoch)):\n",
    "        #shuffle index\n",
    "        np.random.shuffle(rnd_idxs)\n",
    "        for start_idx in list(range(0, N, mb_size)):\n",
    "            # Get minibach\n",
    "            mb_X = X[rnd_idxs[start_idx:start_idx+mb_size]]\n",
    "            mb_Y = one_hot_Y[rnd_idxs[start_idx:start_idx+mb_size]]\n",
    "            \n",
    "            # Forward-prop\n",
    "            As = compute_nnet_outputs(Ws, mb_X, True)\n",
    "            \n",
    "            # Back-prop; on the way, compute each layer's gradient and update its W\n",
    "            Ws=updateWeights(Ws,As,mb_X,mb_Y,learning_rate)   \n",
    "\n",
    "        \n",
    "        # Compute training info, save it, and print it\n",
    "        A = compute_nnet_outputs(Ws, X, False)\n",
    "        cost = np.mean(-np.sum(one_hot_Y * np.log(A), axis=1))\n",
    "        err = np.mean(np.argmax(A, axis=1) != Y.squeeze()) * 100\n",
    "        costs.append(cost)\n",
    "        errs.append(err)\n",
    "        print ('Epoch %d, cost %.3f, err %.3f%%' %(epoch, cost, err))\n",
    "            \n",
    "    return Ws, costs, errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1osNH2dHRsa2"
   },
   "source": [
    "## Chạy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4w9LREcDRsa2"
   },
   "source": [
    "**1.** Đọc dữ liệu (cần đặt thư mục chứa dữ liệu `cifar-10-batches-py` vào cùng thư mục chứa file notebook này)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eE08JoTeRsa3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_train_batchs = 3 \n",
    "train_X, train_Y, test_X, test_Y = read_data('cifar-10-batches-py', num_train_batchs)\n",
    "print ('train_X.shape = %s, train_Y.shape = %s' %(train_X.shape, train_Y.shape))\n",
    "print ('test_X.shape  = %s, test_Y.shape  = %s' %(test_X.shape, test_Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqGz14PrRsa3"
   },
   "source": [
    "**2.** Để thấy được sự ảnh hưởng của số lượng nơ-ron ẩn, ta sẽ lần lượt huấn luyện Neural Net với `hid_layer_sizes = [50]`, `[100]`, và `[200]` (cố định `mb_size = 32`, `learning_rate = 0.01`, `max_epoch = 200`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8hko8LiRsa3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W_0, costs_0, errs_0 = train_nnet(train_X, train_Y, [50], 32, 0.01, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DovPHYXRsa3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W_1, costs_1, errs_1 = train_nnet(train_X, train_Y, [100], 32, 0.01, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Osgo3zorRsa3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W_2, costs_2, errs_2 = train_nnet(train_X, train_Y, [200], 32, 0.01, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QXFoUWrSRsa4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "epochs = np.arange(len(costs_0))\n",
    "plt.figure(figsize=(10,8),dpi=400)\n",
    "plt.plot(epochs, costs_0, label='50 hidden neurons')\n",
    "plt.plot(epochs, costs_1, label='100 hidden neurons')\n",
    "plt.plot(epochs, costs_2, label='200 hidden neurons')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('cost')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('figure_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydpdTguOhud2"
   },
   "source": [
    "Đánh giá mô  hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJgITotrhyd7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Lab03-MachineLearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "103px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "509px",
    "left": "0px",
    "right": "1212px",
    "top": "106px",
    "width": "68px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
